1
00:00:08,939 --> 00:00:14,804
次に、分割されたドキュメントを検索可能な形式に変換するEmbeddingについて見ていきましょう。

2
00:00:16,407 --> 00:00:26,916
Embeddingとは、ドキュメントを数値、すなわちベクトルに変換するプロセスであり、このように変換されたベクトルは高次元空間に配置することができます。

3
00:00:28,056 --> 00:00:40,841
高次元空間に配置することで、意味的に近いもの同士が近くに位置するようになり、この原理を利用して検索を行う方法が、私たちが行おうとしているSemanticSearchです。

4
00:00:42,402 --> 00:00:55,488
OpenAI、Quire、そしてHugging Faceでは様々な事前学習済みEmbeddingモデルが提供されていますが、私たちはAzureが提供しているOpenAI Embeddingモデルを使用します。

5
00:00:56,649 --> 00:01:03,432
各モデルごとにEmbeddingのサイズや次元数も異なり、それに伴うコストも違います。

6
00:01:04,242 --> 00:01:19,272
LargeモデルはSmallモデルに比べてコストが約10倍ほど高いですが、Smallモデルよりも検索性能が高いという社内の実験結果があり、チーム内ではさまざまなプロジェクトでLargeモデルを使用しています。

7
00:01:21,134 --> 00:01:31,918
Embeddingは、文書の内容をベクトルに変換するプロセスです。これにより文書の意味を数値化し、多様な自然言語処理に活用されます。

8
00:01:31,920 --> 00:01:34,549
自然言語処理の作業に活用することができます。

9
00:01:35,822 --> 00:01:49,227
私たちは先ほどお話ししたように、Azureが提供しているOpenAIのエンベディングモデルを使用する予定で、サポートされているモデルはAda、3-small、3-largeの3種類があります。

10
00:01:50,706 --> 00:01:58,271
まず環境変数を設定した後、エンベディングモデルには3-largeモデルを使用します。

11
00:02:01,012 --> 00:02:11,907
サンプル文をテキスト変数に定義し、EmbedQueryという関数を使って与えられたテキストをエンベディングベクトルに変換してみます。

12
00:02:14,389 --> 00:02:23,437
エンベディングされたベクトルの長さは3072であり、この3072はラージモデルの次元数と同じです。

13
00:02:26,300 --> 00:02:43,328
一部の値、最初の5つの値を確認してみると、このような数字の並びであることが分かります。また、テキストだけでなくドキュメントオブジェクト自体もエンベディングすることができます。EmbedQueryではなく、EmbedDocuments関数を使って

14
00:02:45,990 --> 00:02:51,834
テキストのリストを引数として渡し、全体をエンベディング関数に渡してみます。

15
00:02:54,757 --> 00:03:02,360
エンベディングされたベクトルは、入力した4つのテキストが変換されて出力されました。

16
00:03:04,080 --> 00:03:16,283
最初のテキストから5つの要素を抽出してみると、先ほど抽出した数値と同じ値が生成されていることが確認できます。

17
00:03:18,185 --> 00:03:21,846
次は、次元数自体を指定する方法です。

18
00:03:23,985 --> 00:03:29,848
そもそも、Embedding Small、3-Smallモデルは1,536次元の埋め込みを返します。

19
00:03:30,608 --> 00:03:43,991
そして、今私たちが使っているLargeモデルは3,072次元で、先ほどエンベディングされた値を見ても3,072の長さを持つベクトルで作られていることが確認できます。

20
00:03:45,772 --> 00:03:54,235
ここで次元を調整することができ、dimensionを1,024に指定してエンベディングされた値を取り出してみます。

21
00:03:55,534 --> 00:04:00,663
エンベディングモデルを作るときに、次元数を指定してオブジェクトを作成します。

22
00:04:03,085 --> 00:04:16,651
このエンベディングされた1,024次元のモデルにテキストを入力してエンベディングしてみると、その長さが1,024のベクトルが作られていることを確認できます。

23
00:04:18,312 --> 00:04:25,795
次に、このように作られたベクトル同士の類似度を計算してみますが、5つの任意の文章を生成します。

24
00:04:27,576 --> 00:04:34,579
次は、5つの文章を1,024次元にエンベディングするモデルを使って、それぞれをエンベディングしてみます。

25
00:04:36,180 --> 00:04:48,524
次に、コサイン類似度を計算できる関数を作成し、5つの文章間の類似度を比較してみると、次のような結果が得られます。

26
00:04:51,386 --> 00:05:11,511
句点と感嘆符だけが違う2つの文は、類似度がほぼ1に近い0.906という非常に高い値が出ました。一方、『会えてうれしいです』を意味する英語と『私はリンゴが好きです』を意味する英語の類似度は0.4と、非常に低いことが確認できます。

27
00:05:14,052 --> 00:05:20,334
少し前まで、私たちは文書をエンベディングしてベクトルに変換する過程まで確認しました。

28
00:05:21,634 --> 00:05:44,331
これからは、このベクトルたちをうまく整理して、素早く検索できる構造を作る必要があります。それがまさにベクトルインデキシングです。今回ご説明するさまざまなインデキシングアルゴリズムは、実際のコードレベルではパラメータ値として設定するだけでよかったり、あるいはベクトルデータベースで基本的に提供されているアルゴリズムである場合もあります。

29
00:05:45,432 --> 00:06:02,072
しかし、概念を簡単にでも知っているのと全く知らないのとでは大きな違いがあると思うので、気軽に概念だけ理解できるように簡単にまとめてみました。軽く理解していただければ十分かと思います。

30
00:06:03,934 --> 00:06:16,425
Largeモデルでエンベディングを行うと、約3,000次元のベクトルが作られますが、こうした3,000次元のベクトルを無作為に積み重ねておくと、検索するのに非常に多くの時間がかかってしまいます。

31
00:06:18,086 --> 00:06:26,314
そこで、高次元ベクトルデータを効率的かつ体系的に構成し、検索プロセスを最適化する方法がインデックス化です。

32
00:06:28,418 --> 00:06:38,262
類似したデータ同士が近くに配置されるようにグループ化する方法であり、大規模データや複雑なデータでも素早く正確に検索できるようにします。

33
00:06:40,663 --> 00:07:01,036
検索対象となる文書をうまくグルーピングしておき、新しい文が入力されたときに、その新しい文のベクトルが文書が埋め込まれているベクトル空間のどこに位置するのか、最も近い位置にある類似ベクトルは何かを見つけることが、ベクトル検索の核心だと言えます。

34
00:07:03,598 --> 00:07:08,362
まず、Flat Indexは最も単純なベクトルインデックス化の方法です。

35
00:07:09,882 --> 00:07:22,971
この方式はベクトルをそのまま一つ一つ比較する方法であり、ベクトルを変形したりクラスタリングするなどの近似検索を行わないため、最も正確な検索結果を得ることができます。

36
00:07:25,452 --> 00:07:43,526
完全探索、つまり一つ一つ距離を計算して探索する方法なので、100%正確に近いデータを見つけることができます。その代わり、すべてのベクトルと距離を比較しなければならないため、検索時間が長くなり、速度が遅いという欠点があります。

37
00:07:45,447 --> 00:07:52,831
このアルゴリズムは、非常に小規模なデータで非常に精密な検索が必要な場合に使用されることがあります。

38
00:07:54,867 --> 00:07:59,848
次はデータです。Flat Indexよりも高速な検索が可能なIVF方式です。

39
00:08:01,697 --> 00:08:13,005
Flat Indexはすべてのベクトルを一つ一つ比較するため非常に時間がかかりますが、IVFはベクトルをクラスタに分けて検索速度を最適化します。

40
00:08:15,108 --> 00:08:24,233
検索時にはすべてのデータではなく、最も近いクラスタ内だけで検索を行い、一部のクラスタのみで検索することで効率的に検索する方法があります。

41
00:08:24,906 --> 00:08:29,629
比較的に検索対象の数が減るため、より高速に検索できます。

42
00:08:31,509 --> 00:08:36,292
ただし、検索対象が減る分、検索精度が低下するという欠点もあります。

43
00:08:38,653 --> 00:08:41,554
次はIVF PQ方式です。

44
00:08:42,735 --> 00:08:46,618
この方式はIVFをさらに最適化した方法で、

45
00:08:48,759 --> 00:08:57,605
ベクトルを複数のサブベクトルに分割し、それぞれのサブベクトルを量子化してベクトルのサイズを縮小します。

46
00:08:59,885 --> 00:09:16,498
ベクトルのサイズを縮小して演算を最適化することで、さらに高速な検索が可能になります。 要約すると、Flat Indexは非常に正確ですがとても遅い方法であり、IVFは速度が速いものの、精度がやや落ちる場合があります。

47
00:09:17,458 --> 00:09:28,024
IVF PQ方式はさらに高速で、量子化によってメモリも節約できますが、精度はさらに低くなる可能性があります。

48
00:09:29,283 --> 00:09:38,326
最終的には、データの規模や速度、そして精度に応じて、どの方式を使うのが良いか検討する必要があります。

49
00:09:39,547 --> 00:09:49,870
最後にご紹介するのは、HNSWというアルゴリズムです。 まずこの概念を理解する前に、ケビン・ベーコン・ゲームについてお話ししたいと思います。

50
00:09:51,070 --> 00:10:02,647
ケビン・ベーコン・ゲームとは、俳優のケビン・ベーコンを基準にして、6段階以内でハリウッドのすべての俳優がつながっているという概念です。

51
00:10:04,168 --> 00:10:08,071
これと似た例として、チェーンレター（幸運の手紙）も挙げられるかもしれません。

52
00:10:09,491 --> 00:10:21,561
1人が7通ずつ手紙を送ると仮定した場合、たった12回繰り返すだけで、世界中の全人口がこの幸運の手紙を受け取ることができるのです。

53
00:10:24,043 --> 00:10:34,870
このように、少ない段階の探索でほとんどのノードが互いにつながるという概念が「スモールワールド（Small World）」であり、HNSWのSWはこのSmall Worldを指します。

54
00:10:36,251 --> 00:10:42,456
Small Worldの次に、HNSWを理解するためにはスキップリストを理解する必要があります。

55
00:10:43,475 --> 00:10:55,924
スキップリストは、各ノードがデータとポインタを持ち、1列に連結されているリンクリストと配列の長所を組み合わせたデータ構造です。

56
00:10:57,072 --> 00:11:05,779
多層構造になっており、一番下の層にはすべてのデータが含まれ、上の層に行くほど一部のデータが省略されます。

57
00:11:08,361 --> 00:11:17,769
HNSWは、このようなスキップリストの多層構造とNSWのグラフベースの探索方式が組み合わさったアルゴリズムです。

58
00:11:18,811 --> 00:11:37,356
例えば、図書館で本を探す場合を考えてみましょう。1階では探している本がどのカテゴリに属するかを確認し、該当するカテゴリのある階に移動した後、その本がありそうな本棚で本を探すという、このような過程がHNSWアルゴリズムです。

59
00:11:38,158 --> 00:11:49,847
まず最上位レイヤーでエントリーポイントから探索を開始し、現在のレイヤーで検索ノードに最も近いノードへと移動します。

60
00:11:51,349 --> 00:11:55,393
その後、これ以上近づけない場合は、下の階レイヤーに降ります。

61
00:11:57,469 --> 00:12:10,312
次に、そのレイヤーでもターゲットノードに最も近いノードへと移動します。ターゲットノードに近づくまで、またはターゲットノードに到達するまで繰り返し探索を行うアルゴリズムです。

62
00:12:11,832 --> 00:12:22,216
先ほど紹介したアルゴリズムに比べてはるかに速く、正確で効率的なため、多くのベクトルデータベースでもHNSWアルゴリズムが提供されています。

63
00:12:23,035 --> 00:12:29,779
ベクトルデータベースは、エンベディングされたデータを保存し、素早く検索できるようにしてくれるデータベースです。

64
00:12:30,880 --> 00:12:45,072
私たちが今学んだFlat Index、IVF、HNSWのようなインデックス方式を内部的に活用し、ベクトルデータを効率的に管理し検索できるようにしてくれます。

65
00:12:46,394 --> 00:12:50,177
画面でご覧の通り、ベクトルデータベースにはさまざまな種類があります。

66
00:12:51,278 --> 00:13:07,148
それぞれのデータベースごとにサポートしている機能や最適化の方法が異なるため、速度、精度、アルゴリズム、オープンソースかどうかなどを考慮して、実施したいプロジェクトに最も適したベクターデータベースを選定する必要があります。

67
00:13:08,188 --> 00:13:23,017
ここにあるベクターハブのベクターDB比較機能は、さまざまなベクターDBソリューションの機能を比較・検証できるツールなので、どのベクターDBを選ぶか迷ったときには一度ご覧になると良いでしょう。

68
00:13:23,908 --> 00:13:27,272
次に、代表的なベクターDBを比較してみたいと思います。

69
00:13:28,572 --> 00:13:35,038
代表的なオープンソースのベクターDBには、Milvus、Chroma、Qdrantがあります。

70
00:13:36,399 --> 00:13:50,653
MilvusとChromaはPythonをサポートしているため、周辺のプロジェクトや実習でよく使われていることが確認できますし、これら3つのDBすべてがHNSWアルゴリズムをサポートしていることも確認できます。

71
00:13:51,736 --> 00:14:08,041
このほかにも、AI SearchというAzureが提供するサービスでも多く使われており、今日はこの中からオープンソースで、比較的簡単に使えるFaissとChromaの2つのVectorDBを使って実習を行う予定です。

72
00:14:10,162 --> 00:14:12,182
まずはChromaから扱ってみましょう。

73
00:14:13,102 --> 00:14:17,923
Chromaを扱う前に、必要なさまざまな環境変数を設定します。

74
00:14:20,985 --> 00:14:29,772
ベクターDBに格納する前に必要なプロセスである、ドキュメントのロードや分割に必要なツールも一緒にインポートします。

75
00:14:31,053 --> 00:14:38,820
ここではRecursiveCharacterTextSplitterを使用する予定で、チャンクサイズは600、チャンクのオーバーラップは行いません。

76
00:14:40,000 --> 00:14:58,748
実習で使用するファイルは2つのテキストファイルで、NLPに関するキーワードがまとめられたファイルと、ファイナンスに関するキーワードがまとめられたファイルの2種類です。 この2つのファイルをRecursive Text Splitterを使って、ドキュメントをロードしながらチャンクに分割してみましょう。

77
00:15:02,797 --> 00:15:15,062
チャンク数を確認してみると、最初のNLPキーワードファイルでは11個のチャンクが作成され、ファイナンスキーワードのファイルでは6個のチャンクが生成されたことが分かります。

78
00:15:16,763 --> 00:15:22,424
次はエンベディングモデルですが、エンベディングモデルにはLargeモデルを使用します。

79
00:15:26,126 --> 00:15:33,409
これでベクターデータベースを作成するための事前準備はすべて完了し、次はベクターデータベースを作成する段階です。

80
00:15:34,650 --> 00:15:51,440
ベクターデータベースのfrom_documentsクラスメソッドを使うことで、ドキュメントオブジェクトのリストからベクターデータベースを作成することができます。 ドキュメントにはベクターデータベースに保存する文書のリストが入り、エンベディングは私たちが使用するエンベディングモデルです。

81
00:15:53,860 --> 00:16:06,053
PersistDirectoryの場合、パスを指定するとコレクションがそのディレクトリに保存され、ディレクトリが指定されていない場合はデータがメモリに一時的に保存されます。

82
00:16:10,534 --> 00:16:25,119
それでは、先ほど作成したチャンク、最初の文書から作成した11個のチャンクを使い、エンベディングモデルはLargeモデル、そしてコレクション名はmydbという名前でデータベースを作成してみます。

83
00:16:27,879 --> 00:16:33,081
PersistDirectoryを使って、ローカルディスクにファイル形式で保存することもできますし、

84
00:16:37,982 --> 00:16:43,063
逆に、保存されたデータベースをロードすることもできます。

85
00:16:46,464 --> 00:16:49,450
保存されたデータを確認してみると、

86
00:16:51,947 --> 00:17:00,229
最初の文書から作成された11個のチャンクが、このようにデータベース内に格納されていることが確認できます。

87
00:17:03,277 --> 00:17:13,119
もし別の名前のコレクションでロードしようとすると、その名前のコレクションが存在しないため、何の結果も確認できません。

88
00:17:15,259 --> 00:17:22,922
ドキュメントだけでなく、from_textsメソッドを使ってテキストリストからベクターデータベースを作成することもできます。

89
00:17:24,722 --> 00:17:32,704
先ほどと同様に、テキストのリストをインプットとしてChroma DBを作成し、

90
00:17:35,992 --> 00:17:41,474
関連データを確認してみると、次のように出力されることが分かります。

91
00:17:44,194 --> 00:17:52,336
次は類似度検索ですが、ChromaDBではSimilarity Searchを使って類似度検索を行うことができます。

92
00:17:53,377 --> 00:17:58,439
このメソッドは、与えられたクエリと最も類似したドキュメントを返します。

93
00:18:00,059 --> 00:18:22,023
Similarity Searchの中に「TF-IDFについて教えて」というクエリを入力すると、このTF-IDFについて説明している最も類似したドキュメントが検索されます。このとき、検索される結果の数はデフォルトで4つなので、4つのドキュメントが検索されたことが確認できます。

94
00:18:24,865 --> 00:18:27,685
2つに限定して検索することもできます。

95
00:18:29,567 --> 00:18:34,489
フィルターにメタ情報を追加して、検索された結果をフィルタリングすることもできます。

96
00:18:38,361 --> 00:18:46,523
もしソースをフィルタリングする際に、別のソースでフィルタリングすると、何もフィルタリングされていないことが確認できます。

97
00:18:48,824 --> 00:18:54,885
新しく作成するだけでなく、add_documentsを使って追加することもできます。

98
00:18:57,307 --> 00:19:08,784
作成を完了したDB DocというChroma DBに、次のようなコンテンツ内容とメタデータを持つドキュメントを、追加する作業を行ってみます。

99
00:19:12,346 --> 00:19:20,551
IDが1のドキュメントを照会してみると、先ほど上で追加したドキュメントが出力されることを確認できます。

100
00:19:23,471 --> 00:19:27,131
同様に、新しいテキストも追加することができます。

101
00:19:29,637 --> 00:19:42,847
すでにIDが1番のドキュメントは上で追加されていますが、ここで「Add Text」ボタンを押すと、テキスト部分にID1番に関する新しい情報が入力されるため、ドキュメントの内容が上書きされたことを確認できます。

102
00:19:45,950 --> 00:19:59,780
次に、削除も可能で、特定のIDを持つ情報を削除することができ、削除するとIDが1番のドキュメントとその内容がなくなったことを確認できます。

103
00:20:02,305 --> 00:20:07,327
また、特定の条件によってメタデータを照会することもできます。

104
00:20:10,170 --> 00:20:16,054
さらに、このコレクション自体を初期化して、何もない状態にすることもできます。

105
00:20:19,896 --> 00:20:21,577
次はFAISSです。

106
00:20:23,439 --> 00:20:34,801
FaissもChromaと同様に使用することができ、まず環境変数を設定し、実習で使うファイルと

107
00:20:37,721 --> 00:20:47,005
ファイルからテキストを抽出するローダー、そしてテキストからチャンクを作成するスプリッター、必要なライブラリをインポートします。

108
00:20:50,506 --> 00:21:02,288
実習で使用するドキュメントをRecursiveCharacterTextSplitterで分割し、Split 1と2のチャンクとして保存しておきます。

109
00:21:04,805 --> 00:21:12,290
先ほどと同様に、1つ目のドキュメントから11個、2つ目から6個のチャンクが生成されました。

110
00:21:14,732 --> 00:21:25,381
次に、from_documentsメソッドを使って、ドキュメントオブジェクトのリストとエンベディング関数を利用し、ベクターDB Faissを作成してみましょう。

111
00:21:27,142 --> 00:21:30,444
エンベディングモデルは、同様に3-Largeモデルを使用します。

112
00:21:36,846 --> 00:21:46,412
1つ目のドキュメントで作成された11個のチャンクの後ろに、2つ目のドキュメントで作成された6個のチャンクを続けて結合します。

113
00:21:48,192 --> 00:21:56,338
結合した17個のチャンクからなるドキュメントを、エンベディングモデルを通じてFaissに格納してみましょう。

114
00:21:58,660 --> 00:22:16,092
次に、生成されたDB、ベクターデータベースのIDをこのようにしっかりと確認することが可能となっており、具体的には11個と6個、合計17個ものドキュメントが適切に含まれていることがお分かりいただけるのではないかと思います。

115
00:22:18,193 --> 00:22:23,317
このように保存されたドキュメントの属性も確認することができます。

116
00:22:25,097 --> 00:22:34,765
次は同様に、ドキュメントではなくテキスト自体をfrom_textsを使ってテキストリストとしてFaissに保存する方法です。

117
00:22:36,297 --> 00:22:45,740
このように2つの文をリストにして、さらにメタデータも一緒に追加してFaissを作成してみます。

118
00:22:48,560 --> 00:23:02,825
その内容を詳しく確認を行ってみますと、最初の文に対するページコンテンツと、それに対応するメタデータも非常に適切に生成されていることがしっかりと確認することができます。

119
00:23:05,285 --> 00:23:17,367
次は、Similarity Searchを使って与えられたクエリと最も類似したドキュメントを検索する機能で、先ほど見たChromaのSimilarity Searchと同じように利用できます。

120
00:23:19,628 --> 00:23:37,056
同様に、「TF-IDFについて教えて」というクエリを入力した場合、次のように4つのドキュメントが検索され、ここでkを2に設定すると、最も類似度の高い2つの文書だけが出力されることを確認できます。

121
00:23:38,396 --> 00:23:42,941
さらに、メタデータを活用してフィルタリングすることもできます。

122
00:23:47,625 --> 00:23:52,951
また、別のソースで検索すれば、その内容だけを取得可能です。

123
00:23:54,672 --> 00:23:59,757
同様に、ドキュメントを追加したり更新したりすることも可能となっています。

124
00:24:05,144 --> 00:24:08,666
ドキュメントだけでなく、テキストも追加することができます。

125
00:24:12,468 --> 00:24:20,711
そのため、もともと作成していた17個のチャンクの後ろに、今新たに3つのドキュメントが追加されました。

126
00:24:21,571 --> 00:24:24,272
ここでドキュメントを削除してみます。

127
00:24:26,493 --> 00:24:32,096
削除するためのデータ、削除が可能かどうかを確認するためのデータを追加した後に、

128
00:24:37,277 --> 00:24:41,137
削除するIDはdelete1、delete2として作成されました。

129
00:24:45,160 --> 00:24:48,540
インデックスとIDを全体的に確認してみて、

130
00:24:50,882 --> 00:25:01,144
ここで削除するインデックス、削除するIDはdelete1、delete2なので、これらのIDをdelete関数を使って削除してみます。

131
00:25:03,261 --> 00:25:13,688
そして、残っているDBのIDを出力してみると、削除用に入れておいたdelete1、delete2の内容が消えていることが確認できます。

132
00:25:16,469 --> 00:25:18,990
次はローカルに保存する方法です。

133
00:25:21,251 --> 00:25:27,115
パスを指定すれば、作成したベクターデータベースがここに保存されます。

134
00:25:29,396 --> 00:25:46,750
保存されたベクターデータベースは、ローカルでロードを使って今後再利用することができ、ロードされたデータを確認すると、私たちが最近まで使っていたベクターデータベースまできちんと格納されていることが確認できます。
