1
00:00:09,586 --> 00:00:25,992
前回は抽出を学びましたが、今回はテキストを効果的に分割する方法を見ていきましょう。この分割された断片を「チャンク」と呼び、その過程を「チャンク化」と言います。

2
00:00:26,992 --> 00:00:56,646
チャンク化の過程で文書を適切な大きさに分割することで、後の検索性能や応答の正確性、そして品質を高めることができます。むやみに小さな単位で分割すると文脈に関する情報が不足する可能性があり、逆に大きすぎる単位で分割すると文脈は含まれるかもしれませんが、その中から重要な情報を抽出するのが難しくなることがあります。どのように分割するかを考えたとき、最も簡単なのは同じ長さで分ける方法です。

3
00:00:57,707 --> 00:01:07,932
ただし、同じ長さで分割すると文が途中で切れて文脈がつながらなくなることがありますが、これを克服するために登場した概念がOverlapです。

4
00:01:09,063 --> 00:01:21,694
一定部分のチャンクを重ねて分割する方法であり、こうすることで前後のチャンク同士が重なり合い、文脈を一部維持できるという利点があります。

5
00:01:23,194 --> 00:01:39,134
これをより精巧に文書を分割するためには、次のような方法を適用することができます。まず、Semantic Chunkは、単に文字数やトークン数を基準にするのではなく、文の意味的なつながりを考慮して分割する方法です。

6
00:01:40,114 --> 00:01:50,700
通常、文書では同じ話題の内容が段落という単位でまとめられていますが、このように文脈を維持できるように、ある一つの単位でまとめて処理する方法です。

7
00:01:51,981 --> 00:02:01,347
段落単位でチャンクを作成することで、自然に文脈が維持され、文同士の意味的な流れが損なわれないという利点があります。

8
00:02:02,250 --> 00:02:14,883
2番目に、メタデータベースのチャンク化は、チャンク内の情報を補強するために、ページ情報やタイトル、日付、カテゴリなど、さまざまなメタデータを追加する方法です。

9
00:02:15,704 --> 00:02:23,627
検索を行う際、単にテキストだけを検索するのではなく、メタデータまで活用することで、より精密な検索が可能になります。

10
00:02:24,127 --> 00:02:31,590
例えば、論文のタイトル、著者、所属、要約などの内容をメタデータとして活用すれば、

11
00:02:33,789 --> 00:02:40,401
特定の著者の論文だけを検索するなど、検索結果をフィルタリングする際に有効に活用できます。

12
00:02:42,157 --> 00:02:42,781
それでは、

13
00:02:45,081 --> 00:02:47,584
SplitTextを実習してみましょう。

14
00:02:52,204 --> 00:03:01,288
今回は、テキストや情報が抽出された文書から一定の単位でテキストを分割するTextSplitterについて見ていきます。

15
00:03:03,038 --> 00:03:06,181
まず最初は、CharacterTextSplitterです。

16
00:03:06,259 --> 00:03:09,401
この方法は最も基本的な分割方式です。

17
00:03:10,420 --> 00:03:15,864
基本的には、2回の改行を基準に文字単位でテキストを分割します。

18
00:03:17,864 --> 00:03:21,663
今回の実習では、2つのテキストファイルを使用します。

19
00:03:27,227 --> 00:03:39,348
TextSplitterのSeparatorは分割の基準を設定するもので、デフォルトでは2回の改行が使われ、チャンク サイズは各チャンクの最大サイズを設定できます。

20
00:03:40,229 --> 00:03:51,431
そして、Chunk Overlapは隣接するチャンク間でどれだけ重複を許容するかの指標であり、length functionはテキストの長さを計算する関数を指定します。

21
00:03:54,252 --> 00:04:02,997
テキストの長さは210文字に設定し、そしてオーバーラップは一切なしという条件で、スプリッターを新規に作成します。

22
00:04:06,777 --> 00:04:11,218
私たちが実習で使用する2つのテキストファイルを

23
00:04:13,199 --> 00:04:19,184
作成した文字スプリッターを使い、テキストを分割します。

24
00:04:20,725 --> 00:04:31,877
分割された結果を見ると、最初のドキュメントからは32個のチャンクが作成され、2番目のドキュメントからは20個のチャンクが作成されました。

25
00:04:32,959 --> 00:04:42,723
この最初のチャンクの内容には、SemanticSearchからEmbeddingまでの内容が含まれていることが確認できます。

26
00:04:44,545 --> 00:04:58,254
ここにメタデータを追加することができますが、最初のドキュメントには「Document 1」、2番目のドキュメントには「Document 2」というメタデータを追加してみます。

27
00:04:59,997 --> 00:05:16,004
ファイルを読み込んでドキュメントオブジェクトを作成する際、以前はそれぞれ個別に作成していましたが、今回は分割するテキストデータをリスト形式でまとめて渡し、このとき各ドキュメントに対応するメタデータもリストで渡してみます。

28
00:05:18,004 --> 00:05:42,992
すると、全体で52個のチャンクが作成されましたが、この52という数は、最初のドキュメントのチャンク32個と、2番目のドキュメントのチャンク20個が合わさってできたものだと確認できます。また、最後のチャンクを確認したところ、メタデータも適切に「Document 2」と入力されていることが分かります。

29
00:05:44,632 --> 00:05:47,834
次は、RecursiveCharacterTextSplitterです。

30
00:05:50,894 --> 00:05:58,576
一般的によく使われるスプリッターで、文字のリストをパラメータとして受け取り動作します。

31
00:06:01,350 --> 00:06:15,779
チャンクが十分に小さくなるまで、ドキュメントリストにある順番でテキストを分割しようと試みます。基本的に、文字リストは二重改行、一重改行、スペースの順になっています。

32
00:06:17,540 --> 00:06:22,605
つまり、段落、文、単語の順に再帰的に分割する方法です。

33
00:06:24,105 --> 00:06:28,728
先ほど使った appendix keyword というテキストファイルを使ってみましょう。

34
00:06:31,144 --> 00:06:39,807
RecursiveCharacterTextSplitter も同様に、チャンクサイズとオーバーラップ、そして length function を指定します。

35
00:06:42,208 --> 00:06:47,509
チャンクサイズは52、重なりはまったくない状態で、スプリッターの設定を行います。

36
00:06:50,029 --> 00:06:55,471
このように、TextSplitterで分割されたチャンクを確認してみると

37
00:06:58,112 --> 00:07:00,552
このように分割されたことを確認できます。

38
00:07:01,928 --> 00:07:24,041
まず、2回の改行で分割されたため、SemanticSearchが1つのチャンクになり、その次は文ごとに分割されます。定義が書かれた1つの文が50文字を超えるため、スペースを基準に最大50文字になるように分割されたことが分かります。

39
00:07:25,822 --> 00:07:28,105
次はTokenTextSplitterです。

40
00:07:28,944 --> 00:07:33,646
言語モデルにはトークンの制限があるため、その制限を超えないようにしなければなりません。

41
00:07:35,507 --> 00:07:43,009
このようなトークンスプリッターは、テキストをトークン単位でチャンク化する際に便利に使うことができます。

42
00:07:45,470 --> 00:07:49,990
まず、OpenAIが作った『tiktoken』というトークナイザーを使用します。

43
00:07:51,350 --> 00:07:56,793
先ほど使用したテキストファイルとキャラクターテキストファイルを読み込みます。

44
00:07:56,973 --> 00:08:08,257
スプリッターを使ってテキストを分割しますが、このときtiktokenを使ってテキストを分割するfromTikTokenEncoderメソッドを通じてスプリッターを設定します。

45
00:08:10,418 --> 00:08:17,002
このように分割されたチャンクの長さを確認すると、合計51個のチャンクに分割されたことが分かります。

46
00:08:19,062 --> 00:08:27,865
同様に、TokenTextSplitterを使っても、先ほどと同じようにトークン単位で分割します。テキストを分割することができます。

47
00:08:30,646 --> 00:08:32,408
次はSemanticChunkerです。

48
00:08:33,687 --> 00:08:42,552
SemanticChunkerは、LangChainで実験的に提供されている機能で、テキストを意味的な類似性に基づいて分割する方法です。

49
00:08:44,352 --> 00:08:56,033
テキストを文単位で分割した後、3文ずつグループ化してEmbeddingし、高次元空間で類似した文を統合するという一連のプロセスを経ます。

50
00:08:58,813 --> 00:09:05,615
このようにすると、テキストデータは意味的に類似し、関連性のあるチャンクに分割されます。

51
00:09:08,197 --> 00:09:16,360
まず、同じように実習で使うファイルを読み込んだ後、SemanticChunkerを使ってみます。

52
00:09:17,240 --> 00:09:27,765
SemanticChunkerではEmbeddingを使う必要があるため、事前に行ったのと同様に環境変数やEmbeddingモデルの設定を行います。

53
00:09:29,466 --> 00:09:31,707
このとき、Largeモデルを使用します。

54
00:09:34,067 --> 00:09:38,969
SemanticChunkerをEmbeddingモデルと一緒にSplitterで作成した後、

55
00:09:41,889 --> 00:09:45,250
使用するファイルを分割すると、

56
00:09:47,711 --> 00:09:51,673
下のように分割されたチャンクを確認できます。

57
00:09:52,639 --> 00:10:01,563
最初のチャンクでは、SemanticSearchからWord2Vecの定義までが1つのチャンクにまとめられていることが分かります。

58
00:10:03,845 --> 00:10:13,190
さらに、2ページ目のコンテンツを確認すると、Faissに関する情報とオープンソースが1つのチャンクにまとめられていることが分かります。

59
00:10:14,831 --> 00:10:34,043
最後に紹介するのは、MarkdownHeaderTextSplitterです。Markdownはヘッダーによって文書の構造を把握できるファイル形式であり、このような文書の全体的な文脈や構造を考慮して意味のある方法で処理すれば、効果的に文書を分割することができます。

60
00:10:35,344 --> 00:10:50,729
文書内でそれぞれのヘッダーの下にある内容を基にチャンクを作成すれば、文脈を維持しつつ構造を効果的に活用することができ、その際に利用できるツールがMarkdownHeaderTextSplitterです。

61
00:10:52,009 --> 00:11:00,700
文書を指定されたヘッダーの集合に従って分割することができ、これによって文書全体の構造を保ったまま内容を扱うことが可能になります。

62
00:11:02,961 --> 00:11:09,205
まず、Markdown形式の文書を文字列として定義した後、

63
00:11:12,027 --> 00:11:19,051
Markdown文書を分割するヘッダーのレベルと、そのレベルの名前をタプル形式のリストで指定します。

64
00:11:20,772 --> 00:11:30,272
シャープで示されるheader 1にはheader 1という名前を付け、シャープが2つのheader 2にはheader 2という名前を付けます。

65
00:11:32,374 --> 00:11:42,559
このように定義したMarkdownヘッダーを基準として、テキストを分割するためのテキストスプリッターを実際に作成していきます。

66
00:11:45,400 --> 00:11:53,466
Markdown形式のテキストをこのテキストスプリッターで分割すると、次のような結果が得られます。

67
00:11:57,789 --> 00:12:04,095
基本的にMarkdownスプリッターは、分割されるヘッダーを出力チャンクの内容から削除しますが、

68
00:12:06,236 --> 00:12:14,903
このヘッダーの内容を含めるためには、StripHeadersというオプションをfalseに設定して無効化することができます。

69
00:12:15,745 --> 00:12:19,528
無効化した状態でもう一度Markdownドキュメントを分割してみると、

70
00:12:22,110 --> 00:12:32,200
ヘッダーに関する情報が含まれたメタデータだけでなく、これらのヘッダーの内容もチャンクの一部として含まれて分割されることが確認できます。

71
00:12:35,642 --> 00:12:47,047
Markdownグループ内では、追加のTextSplitterを適用することができますので、もう少し長い内容のMarkdown形式のテキストを指定してみます。

72
00:12:49,628 --> 00:13:00,629
その後、同様にヘッダーレベルと名前を指定し、ヘッダーレベルに従って分割すると、次のようにヘッダーを基準に分割されます。

73
00:13:03,951 --> 00:13:13,975
ここで、実際のコンテンツの内容が長い場合は、この内容をさらに細かく分割してチャンクを作成する必要があります。

74
00:13:16,798 --> 00:13:20,419
ここでは、RecursiveCharacterTextSplitterを使用します。

75
00:13:24,857 --> 00:13:46,635
MarkdownHeaderTextsplitterを使ってヘッダーを基準に分割した内容を、さらにRecursiveで追加的に分割する方法です。分割された内容を見ると、ヘッダー1、ヘッダー2の中にあるコンテンツが2つに分かれ、合計3つのコンテンツに分割されていることが確認できます。

76
00:13:48,457 --> 00:13:56,423
単に1つのSplitterを使うのではなく、自分が処理すべき文書を最も完璧に処理できる方法のために、

77
00:13:58,505 --> 00:14:06,871
文書の特性を最もよく反映できるように、どのように分割するかを考え、どんなメタデータを活用できるかも検討する必要があります。

78
00:14:09,193 --> 00:14:14,258
さまざまな観点から多くの検討が必要なプロセスが、TextSplitterの工程です。
